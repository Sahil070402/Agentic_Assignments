# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EBd9cdieoknnlA_4UyRAeSE5iMPeVX04
"""

from google.colab import files
uploaded = files.upload()

! pip install "unstructured[all-docs]" pillow pydantic lxml pillow matplotlib

!sudo apt-get update

!sudo apt-get install poppler-utils

!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn

!pip install unstructured-pytesseract
!pip install tesseract-ocr

!pip install unstructured "unstructured[pdf]" langchain langchain-openai langchain-milvus rank-bm25 python-docx numpy pillow opencv-python pymilvus pdfplumber PyMuPDF chromadb langchain-chroma

!pip install langchain-google-genai
!pip install langchain-experimental

pip install dot env

import os
import time
from unstructured.partition.pdf import partition_pdf
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain, RetrievalQA
from langchain_milvus import Milvus
from rank_bm25 import BM25Okapi
from docx import Document
from dotenv import load_dotenv
import numpy as np

pdf_files = ["/content/Data/file1.pdf", "/content/Data/file2.pdf","/content/Data/file3.pdf"]  # Replace with actual PDF paths
all_elements = []
for pdf in pdf_files:
    elements = partition_pdf(
        filename=pdf,
        strategy="hi_res",
        extract_images_in_pdf=True,
        extract_image_block_types=["Image", "Table"],
        extract_image_block_to_payload=False,
        extract_image_block_output_dir="extracted_data"
    )
    all_elements.extend(elements)

all_elements

Header=[]
Footer=[]
Title=[]
NarrativeText=[]
Text=[]
ListItem=[]
for element in all_elements:
  if "unstructured.documents.elements.Header" in str(type(element)):
            Header.append(str(element))
  elif "unstructured.documents.elements.Footer" in str(type(element)):
            Footer.append(str(element))
  elif "unstructured.documents.elements.Title" in str(type(element)):
            Title.append(str(element))
  elif "unstructured.documents.elements.NarrativeText" in str(type(element)):
            NarrativeText.append(str(element))
  elif "unstructured.documents.elements.Text" in str(type(element)):
            Text.append(str(element))
  elif "unstructured.documents.elements.ListItem" in str(type(element)):
            ListItem.append(str(element))

len(NarrativeText)

tab=[]
for element in all_elements:
  if "unstructured.documents.elements.Table" in str(type(element)):
            tab.append(str(element))

len(tab)

tab[0]

tab[1]

img=[]
for element in all_elements:
  if "unstructured.documents.elements.Image" in str(type(element)):
            img.append(str(element))

len(img)

img[3]

model = ChatOpenAI(
    model="gpt-4o-2024-08-06",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    openai_api_base=os.getenv("BASE_URL"),
)

model

embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

embeddings

from langchain_experimental.text_splitter import SemanticChunker

# Initialize LLM for summarization
llm = model
prompt = ChatPromptTemplate.from_template("Create a concise summary of the following content for efficient retrieval, capturing key concepts and entities: {element}")
summarize_chain = prompt | llm

# Summarize NarrativeText
text_summaries = summarize_chain.batch(NarrativeText, {"max_concurrency": 5})

# Extract text content from AIMessage objects
text_summaries_str = [summary.content if hasattr(summary, 'content') else str(summary) for summary in text_summaries]

# Initialize embeddings for SemanticChunker
embeddings = embeddings

# Semantic chunking with smaller batches, longer delays, and logging
import logging

# Set up logging to track embedding requests
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile", breakpoint_threshold_amount=95)
text_chunks = []
batch_size = 5  # Smaller batch size to reduce API calls
request_count = 0
start_time = time.time()

for i in range(0, len(text_summaries_str), batch_size):
    batch = text_summaries_str[i:i + batch_size]
    logger.info(f"Processing batch {i // batch_size + 1} with {len(batch)} summaries")
    for summary in batch:
        try:
            chunks = text_splitter.split_text(summary)
            text_chunks.extend(chunks)
            request_count += 1  # Increment request count (approximation)
            logger.info(f"Processed summary {i + 1}/{len(text_summaries_str)}, total requests: {request_count}")
        except Exception as e:
            logger.error(f"Error processing summary {i + 1}: {str(e)}")
    # Calculate delay to stay under 150 requests/minute (2.5 requests/second)
    time.sleep(3)  # 3-second delay per batch of 5 summaries (~100 requests/minute)

elapsed_time = time.time() - start_time
logger.info(f"Total text chunks: {len(text_chunks)}")
logger.info(f"Processed {request_count} requests in {elapsed_time:.2f} seconds")

print(f"Total text chunks: {len(text_chunks)}")



type(text_summaries_str)

len(text_chunks)

import base64
from langchain.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage
import os

# Function to encode images to base64
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# Define prompts for images and tables
image_prompt_text = """You are an assistant tasked with summarizing images for retrieval. \
These summaries will be embedded and used to retrieve the raw image. \
Give a concise summary of the image that is well optimized for retrieval."""
image_prompt = ChatPromptTemplate.from_template(image_prompt_text)

table_prompt_text = """You are an assistant tasked with summarizing tables for retrieval. \
These summaries will be embedded and used to retrieve the raw table. \
Give a concise summary of the table that is well optimized for retrieval."""
table_prompt = ChatPromptTemplate.from_template(table_prompt_text)

# Summarize images and tables
def image_summarize(img_base64, prompt_template):
    vision_llm = model  # Using your model (GPT-4o with vision capability)
    prompt = prompt_template.format()  # No variables to fill in this case
    # Use HumanMessage to structure the input for LangChain
    message = HumanMessage(
        content=[
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_base64}"}
        ]
    )
    response = vision_llm.invoke([message])
    return response.content

image_summaries = []
table_summaries = []
for img_file in os.listdir("extracted_data"):
    if img_file.endswith(".jpg"):
        img_path = os.path.join("extracted_data", img_file)
        base64_image = encode_image(img_path)
        if "figure" in img_file.lower():
            summary = image_summarize(base64_image, image_prompt)
            image_summaries.append(summary)
        elif "table" in img_file.lower():
            summary = image_summarize(base64_image, table_prompt)
            table_summaries.append(summary)

print(f"Image summaries: {len(image_summaries)}")
print(f"Table summaries: {len(table_summaries)}")

type(image_summaries)

type(table_summaries)

!pip install faiss-cpu  # Use faiss-gpu if you have a GPU
!pip install langchain-community  # For FAISS integration with LangChain

from langchain_community.vectorstores import FAISS
from langchain.storage import InMemoryStore
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain_core.documents import Document
import uuid
import os

# Combine all summaries
all_summaries = text_chunks + image_summaries + table_summaries

# Create document IDs for each entry
doc_ids = [str(uuid.uuid4()) for _ in range(len(all_summaries))]

# Create documents for the vector store (these will contain the summaries)
summary_documents = []
raw_documents = {}  # Dictionary to store raw documents by ID

for i, summary in enumerate(all_summaries):
    doc_id = doc_ids[i]

    if i < len(text_chunks):
        # For text chunks
        summary_documents.append(Document(
            page_content=summary,
            metadata={"doc_id": doc_id, "type": "text"}
        ))
        raw_documents[doc_id] = Document(
            page_content=text_chunks[i],
            metadata={"type": "text"}
        )

    elif i < len(text_chunks) + len(image_summaries):
        # For images
        img_idx = i - len(text_chunks)
        img_files = [f for f in os.listdir("extracted_data") if "figure" in f.lower()]

        if img_idx < len(img_files):
            img_file = img_files[img_idx]
            img_path = os.path.join("extracted_data", img_file)
            base64_image = encode_image(img_path)

            summary_documents.append(Document(
                page_content=summary,
                metadata={"doc_id": doc_id, "type": "image", "filename": img_file}
            ))
            raw_documents[doc_id] = Document(
                page_content=base64_image,
                metadata={"type": "image", "filename": img_file}
            )
    else:
        # For tables
        table_idx = i - len(text_chunks) - len(image_summaries)
        table_files = [f for f in os.listdir("extracted_data") if "table" in f.lower()]

        if table_idx < len(table_files):
            table_file = table_files[table_idx]
            table_path = os.path.join("extracted_data", table_file)
            base64_table = encode_image(table_path)

            summary_documents.append(Document(
                page_content=summary,
                metadata={"doc_id": doc_id, "type": "table", "filename": table_file}
            ))
            raw_documents[doc_id] = Document(
                page_content=base64_table,
                metadata={"type": "table", "filename": table_file}
            )

# Create the store with the correct class
store = InMemoryStore()
store.mset(list(raw_documents.items()))

# Define the path for saving FAISS vector store
FAISS_INDEX_PATH = "faiss_vector_store"

# Check if FAISS vector store already exists locally
if os.path.exists(FAISS_INDEX_PATH):
    print("Loading existing FAISS vector store...")
    vectorstore = FAISS.load_local(
        folder_path=FAISS_INDEX_PATH,
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )
    print("FAISS vector store loaded successfully!")
else:
    print("Creating new FAISS vector store...")
    # Create FAISS vector store from the summary documents
    vectorstore = FAISS.from_documents(
        documents=summary_documents,
        embedding=embeddings
    )

    # Save the vector store locally
    vectorstore.save_local(FAISS_INDEX_PATH)
    print(f"FAISS vector store saved to: {FAISS_INDEX_PATH}")

# Create a MultiVectorRetriever
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key="doc_id",
    search_kwargs={"k": 5}
)

print("Multi-vector store created with FAISS and InMemoryStore!")
print(f"Total documents indexed: {len(summary_documents)}")
print(f"Raw documents stored: {len(raw_documents)}")

# Step 5: Create FAISS Vector Stores with Different Index Types
import faiss
import numpy as np
import time
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

print("Creating FAISS vector stores with different index types...")

# Get embeddings for all summary documents (we'll use the same documents you created)
print("Getting embeddings for summary documents...")
doc_embeddings = []
for doc in summary_documents:
    # Get embedding for each document
    embedding = embeddings.embed_query(doc.page_content)
    doc_embeddings.append(embedding)

doc_embeddings = np.array(doc_embeddings).astype('float32')
print(f"Created embeddings array with shape: {doc_embeddings.shape}")

# Dictionary to store different FAISS stores
faiss_stores = {}

# 1. FLAT Index (Exact search)
print("\n1. Creating FLAT index...")
start_time = time.time()

# Create FLAT index
flat_index = faiss.IndexFlatIP(doc_embeddings.shape[1])  # Inner Product for cosine similarity
flat_index.add(doc_embeddings)

# Create FAISS store with FLAT index
faiss_stores['FLAT'] = FAISS(
    embedding_function=embeddings.embed_query,
    index=flat_index,
    docstore=vectorstore.docstore,
    index_to_docstore_id=vectorstore.index_to_docstore_id
)

flat_time = time.time() - start_time
print(f"FLAT index created in {flat_time:.2f} seconds")

# 2. HNSW Index (Hierarchical Navigable Small World)
print("\n2. Creating HNSW index...")
start_time = time.time()

# Create HNSW index
hnsw_index = faiss.IndexHNSWFlat(doc_embeddings.shape[1], 32)  # 32 is M parameter
hnsw_index.hnsw.efConstruction = 200  # Construction parameter
hnsw_index.add(doc_embeddings)

# Create FAISS store with HNSW index
faiss_stores['HNSW'] = FAISS(
    embedding_function=embeddings.embed_query,
    index=hnsw_index,
    docstore=vectorstore.docstore,
    index_to_docstore_id=vectorstore.index_to_docstore_id
)

hnsw_time = time.time() - start_time
print(f"HNSW index created in {hnsw_time:.2f} seconds")

# 3. IVF Index (Inverted File)
print("\n3. Creating IVF index...")
start_time = time.time()

# Create IVF index
nlist = min(100, len(doc_embeddings) // 4)  # Number of clusters
quantizer = faiss.IndexFlatIP(doc_embeddings.shape[1])
ivf_index = faiss.IndexIVFFlat(quantizer, doc_embeddings.shape[1], nlist)

# Train the index (required for IVF)
ivf_index.train(doc_embeddings)
ivf_index.add(doc_embeddings)

# Create FAISS store with IVF index
faiss_stores['IVF'] = FAISS(
    embedding_function=embeddings.embed_query,
    index=ivf_index,
    docstore=vectorstore.docstore,
    index_to_docstore_id=vectorstore.index_to_docstore_id
)

ivf_time = time.time() - start_time
print(f"IVF index created in {ivf_time:.2f} seconds")

print(f"\n✅ All FAISS stores created successfully!")
print(f"Available indices: {list(faiss_stores.keys())}")

# Display index creation times
creation_times = {
    'FLAT': flat_time,
    'HNSW': hnsw_time,
    'IVF': ivf_time
}

print("\nIndex Creation Times:")
for idx_type, time_taken in creation_times.items():
    print(f"  {idx_type}: {time_taken:.2f} seconds")

# Step 6: Create MultiVector Retriever Pipeline with Different FAISS Indices
import time
from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryStore

class MultiVectorRAGPipeline:
    def __init__(self, faiss_stores: Dict, raw_documents: Dict, k: int = 5):
        """
        Initialize the MultiVector retriever pipeline with different FAISS stores

        Args:
            faiss_stores: Dictionary containing different FAISS stores (with summaries)
            raw_documents: Dictionary containing raw documents (original content)
            k: Number of documents to retrieve
        """
        self.faiss_stores = faiss_stores
        self.raw_documents = raw_documents
        self.k = k
        self.multivector_retrievers = {}

        # Create MultiVector retrievers for each index type
        for index_type, vectorstore in faiss_stores.items():
            # Create a new docstore for each retriever with raw documents
            docstore = InMemoryStore()
            docstore.mset(list(raw_documents.items()))

            # Create MultiVector retriever
            self.multivector_retrievers[index_type] = MultiVectorRetriever(
                vectorstore=vectorstore,
                docstore=docstore,
                id_key="doc_id",
                search_kwargs={"k": k}
            )

        print(f"✅ Created MultiVector retrievers for: {list(self.multivector_retrievers.keys())}")
        print(f"📄 Raw documents in docstore: {len(raw_documents)}")

    def retrieve_documents(self, query: str, index_type: str = None) -> Dict[str, Any]:
        """
        Retrieve raw documents using MultiVector retrieval with specified index type

        Args:
            query: Search query
            index_type: Specific index type to use (None for all)

        Returns:
            Dictionary with retrieval results and timing
        """
        results = {}

        if index_type:
            # Use specific index type
            if index_type not in self.multivector_retrievers:
                raise ValueError(f"Index type '{index_type}' not available")

            start_time = time.time()
            # This retrieves summaries but returns RAW documents
            raw_docs = self.multivector_retrievers[index_type].get_relevant_documents(query)
            retrieval_time = time.time() - start_time

            results[index_type] = {
                'raw_documents': raw_docs,  # These are the original documents (text, images, tables)
                'retrieval_time': retrieval_time,
                'num_docs': len(raw_docs)
            }
        else:
            # Use all index types
            for idx_type, retriever in self.multivector_retrievers.items():
                start_time = time.time()
                # This searches summaries but returns RAW documents
                raw_docs = retriever.get_relevant_documents(query)
                retrieval_time = time.time() - start_time

                results[idx_type] = {
                    'raw_documents': raw_docs,  # Original content for LLM generation
                    'retrieval_time': retrieval_time,
                    'num_docs': len(raw_docs)
                }

        return results

    def get_summary_and_raw(self, query: str, index_type: str) -> Dict[str, Any]:
        """
        Get both summary matches and corresponding raw documents for analysis

        Args:
            query: Search query
            index_type: Index type to use

        Returns:
            Dictionary with both summary and raw document information
        """
        if index_type not in self.faiss_stores:
            raise ValueError(f"Index type '{index_type}' not available")

        # Get summary documents (what's actually searched)
        start_time = time.time()
        summary_docs = self.faiss_stores[index_type].similarity_search(query, k=self.k)

        # Get corresponding raw documents
        raw_docs = self.multivector_retrievers[index_type].get_relevant_documents(query)
        retrieval_time = time.time() - start_time

        return {
            'summary_documents': summary_docs,     # What was searched
            'raw_documents': raw_docs,             # What will be sent to LLM
            'retrieval_time': retrieval_time,
            'workflow': 'Query → Search Summaries → Return Raw Content'
        }

    def batch_retrieve(self, queries: List[str]) -> Dict[str, Dict]:
        """
        Perform batch MultiVector retrieval for multiple queries
        """
        batch_results = {}

        for idx_type in self.multivector_retrievers.keys():
            batch_results[idx_type] = {
                'total_time': 0,
                'average_time': 0,
                'queries_processed': 0,
                'results': []
            }

        for query in queries:
            query_results = self.retrieve_documents(query)

            for idx_type, result in query_results.items():
                batch_results[idx_type]['total_time'] += result['retrieval_time']
                batch_results[idx_type]['queries_processed'] += 1
                batch_results[idx_type]['results'].append({
                    'query': query,
                    'raw_documents': result['raw_documents'],
                    'retrieval_time': result['retrieval_time']
                })

        # Calculate averages
        for idx_type in batch_results.keys():
            if batch_results[idx_type]['queries_processed'] > 0:
                batch_results[idx_type]['average_time'] = (
                    batch_results[idx_type]['total_time'] /
                    batch_results[idx_type]['queries_processed']
                )

        return batch_results

# Initialize the MultiVector RAG pipeline
multivector_pipeline = MultiVectorRAGPipeline(faiss_stores, raw_documents, k=5)

# Test queries for evaluation
test_queries = [
    "What are the main findings discussed in the documents?",
    "Can you explain the methodology used?",
    "What are the key results and conclusions?",
    "Tell me about the data analysis performed",
    "What recommendations are provided?"
]

print(f"✅ MultiVector RAG pipeline created successfully!")
print(f"📊 Test queries prepared: {len(test_queries)}")
print(f"🔍 Available MultiVector retrievers: {list(multivector_pipeline.multivector_retrievers.keys())}")

# Test the MultiVector workflow
print("\n🧪 Testing MultiVector workflow with first query...")
test_result = multivector_pipeline.get_summary_and_raw(test_queries[0], 'FLAT')

print(f"\n📋 MultiVector Workflow Results:")
print(f"  Workflow: {test_result['workflow']}")
print(f"  Summary docs found: {len(test_result['summary_documents'])}")
print(f"  Raw docs returned: {len(test_result['raw_documents'])}")
print(f"  Retrieval time: {test_result['retrieval_time']:.4f} seconds")

# Show document types in raw results
doc_types = {}
for doc in test_result['raw_documents']:
    doc_type = doc.metadata.get('type', 'unknown')
    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1

print(f"  Raw document types: {doc_types}")

print("\n🎯 This is TRUE MultiVector RAG:")
print("  ✓ Searches summaries (optimized for retrieval)")
print("  ✓ Returns raw content (optimized for generation)")
print("  ✓ Includes text, images (base64), and tables")

# Step 7: Check Retriever Time (Performance Comparison)
import time
import statistics
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List

class PerformanceBenchmark:
    def __init__(self, multivector_pipeline):
        """
        Initialize performance benchmark

        Args:
            multivector_pipeline: MultiVectorRAGPipeline instance
        """
        self.pipeline = multivector_pipeline
        self.results = {}

    def single_query_benchmark(self, query: str, iterations: int = 10) -> Dict:
        """
        Benchmark a single query across all index types

        Args:
            query: Query to test
            iterations: Number of iterations for averaging

        Returns:
            Dictionary with performance results
        """
        print(f"🔍 Benchmarking query: '{query[:50]}...'")

        results = {}

        for index_type in self.pipeline.multivector_retrievers.keys():
            times = []

            print(f"  Testing {index_type}...")

            for i in range(iterations):
                start_time = time.time()
                docs = self.pipeline.multivector_retrievers[index_type].get_relevant_documents(query)
                end_time = time.time()

                times.append(end_time - start_time)

                # Small delay to avoid overwhelming the system
                time.sleep(0.01)

            # Calculate statistics
            results[index_type] = {
                'times': times,
                'mean_time': statistics.mean(times),
                'median_time': statistics.median(times),
                'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
                'min_time': min(times),
                'max_time': max(times),
                'num_docs_retrieved': len(docs)
            }

            print(f"    Average: {results[index_type]['mean_time']:.4f}s")

        return results

    def batch_benchmark(self, queries: List[str], iterations: int = 5) -> Dict:
        """
        Benchmark multiple queries across all index types

        Args:
            queries: List of queries to test
            iterations: Number of iterations per query

        Returns:
            Comprehensive benchmark results
        """
        print(f"🚀 Running batch benchmark with {len(queries)} queries, {iterations} iterations each...")

        all_results = {}

        for i, query in enumerate(queries):
            print(f"\n📋 Query {i+1}/{len(queries)}")
            query_results = self.single_query_benchmark(query, iterations)
            all_results[f"Query_{i+1}"] = query_results

        # Aggregate results
        aggregated = self._aggregate_results(all_results)

        return {
            'individual_queries': all_results,
            'aggregated': aggregated
        }

    def _aggregate_results(self, all_results: Dict) -> Dict:
        """
        Aggregate results across all queries
        """
        aggregated = {}

        for index_type in self.pipeline.multivector_retrievers.keys():
            all_times = []

            for query_key, query_results in all_results.items():
                all_times.extend(query_results[index_type]['times'])

            aggregated[index_type] = {
                'total_queries': len(all_results),
                'total_retrievals': len(all_times),
                'mean_time': statistics.mean(all_times),
                'median_time': statistics.median(all_times),
                'std_dev': statistics.stdev(all_times) if len(all_times) > 1 else 0,
                'min_time': min(all_times),
                'max_time': max(all_times),
                'throughput_qps': len(all_times) / sum(all_times)  # Queries per second
            }

        return aggregated

    def create_performance_report(self, benchmark_results: Dict) -> pd.DataFrame:
        """
        Create a detailed performance report
        """
        aggregated = benchmark_results['aggregated']

        # Create DataFrame
        df_data = []
        for index_type, stats in aggregated.items():
            df_data.append({
                'Index_Type': index_type,
                'Mean_Time_ms': stats['mean_time'] * 1000,
                'Median_Time_ms': stats['median_time'] * 1000,
                'Std_Dev_ms': stats['std_dev'] * 1000,
                'Min_Time_ms': stats['min_time'] * 1000,
                'Max_Time_ms': stats['max_time'] * 1000,
                'Throughput_QPS': stats['throughput_qps'],
                'Total_Retrievals': stats['total_retrievals']
            })

        df = pd.DataFrame(df_data)
        df = df.sort_values('Mean_Time_ms')  # Sort by performance

        return df

    def plot_performance_comparison(self, benchmark_results: Dict):
        """
        Create visualizations of performance comparison
        """
        df = self.create_performance_report(benchmark_results)

        # Create subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('FAISS Index Performance Comparison', fontsize=16, fontweight='bold')

        # 1. Mean retrieval time
        bars1 = ax1.bar(df['Index_Type'], df['Mean_Time_ms'],
                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax1.set_title('Mean Retrieval Time')
        ax1.set_ylabel('Time (milliseconds)')
        ax1.set_xlabel('Index Type')

        # Add value labels on bars
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}ms', ha='center', va='bottom')

        # 2. Throughput (Queries per second)
        bars2 = ax2.bar(df['Index_Type'], df['Throughput_QPS'],
                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax2.set_title('Throughput (Queries per Second)')
        ax2.set_ylabel('Queries per Second')
        ax2.set_xlabel('Index Type')

        for bar in bars2:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}', ha='center', va='bottom')

        # 3. Time distribution (box plot)
        all_times_data = []
        labels = []
        for index_type in df['Index_Type']:
            # Get all times for this index type
            times = []
            for query_results in benchmark_results['individual_queries'].values():
                times.extend([t * 1000 for t in query_results[index_type]['times']])
            all_times_data.append(times)
            labels.append(index_type)

        ax3.boxplot(all_times_data, labels=labels)
        ax3.set_title('Time Distribution (Box Plot)')
        ax3.set_ylabel('Time (milliseconds)')
        ax3.set_xlabel('Index Type')

        # 4. Performance ranking
        df_sorted = df.sort_values('Mean_Time_ms')
        colors = ['#2ECC71', '#F39C12', '#E74C3C']  # Green, Orange, Red
        bars4 = ax4.barh(range(len(df_sorted)), df_sorted['Mean_Time_ms'],
                        color=colors[:len(df_sorted)])
        ax4.set_yticks(range(len(df_sorted)))
        ax4.set_yticklabels(df_sorted['Index_Type'])
        ax4.set_title('Performance Ranking (Lower is Better)')
        ax4.set_xlabel('Mean Time (milliseconds)')

        # Add ranking labels
        for i, (idx, row) in enumerate(df_sorted.iterrows()):
            ax4.text(row['Mean_Time_ms'] + 0.1, i,
                    f"#{i+1}: {row['Mean_Time_ms']:.2f}ms",
                    va='center')

        plt.tight_layout()
        plt.show()

        return fig

# Initialize benchmark
benchmark = PerformanceBenchmark(multivector_pipeline)

# Run comprehensive benchmark
print("🔥 Starting comprehensive performance benchmark...")
print("This will test all three index types (FLAT, HNSW, IVF) with multiple queries")

benchmark_results = benchmark.batch_benchmark(test_queries, iterations=10)

# Create performance report
print("\n📊 Creating performance report...")
performance_df = benchmark.create_performance_report(benchmark_results)

print("\n🏆 PERFORMANCE RESULTS:")
print("="*80)
print(performance_df.to_string(index=False))

# Find the fastest index
fastest_index = performance_df.iloc[0]['Index_Type']
fastest_time = performance_df.iloc[0]['Mean_Time_ms']

print(f"\n🥇 FASTEST INDEX: {fastest_index}")
print(f"   Average retrieval time: {fastest_time:.2f} milliseconds")
print(f"   Throughput: {performance_df.iloc[0]['Throughput_QPS']:.1f} queries/second")

# Create visualizations
print("\n📈 Generating performance visualizations...")
benchmark.plot_performance_comparison(benchmark_results)

print("\n✅ Performance benchmark completed!")
print(f"🎯 Recommendation: Use {fastest_index} index for best performance")

# Step 9: Implement Reranking using BM25 and MMR
import time
from rank_bm25 import BM25Okapi
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.documents import Document
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RerankedRAGPipeline:
    def __init__(self, multivector_pipeline, llm_model):
        """
        Enhanced RAG pipeline with reranking capabilities

        Args:
            multivector_pipeline: Your existing MultiVectorRAGPipeline
            llm_model: LLM model for reranking and generation
        """
        self.pipeline = multivector_pipeline
        self.llm = llm_model
        self.bm25_models = {}
        self.embeddings = embeddings  # Your existing embeddings

        # Initialize BM25 models for each index type
        self._initialize_bm25_models()

    def _initialize_bm25_models(self):
        """Initialize BM25 models for hybrid search"""
        print("🔧 Initializing BM25 models for hybrid search...")

        # Get all documents from your raw_documents
        all_docs = list(self.pipeline.raw_documents.values())

        # Prepare corpus for BM25 (text content only)
        corpus = []
        for doc in all_docs:
            if doc.metadata.get('type') == 'text':
                # Tokenize the text content
                tokens = doc.page_content.lower().split()
                corpus.append(tokens)
            else:
                # For images and tables, use empty tokens or metadata
                corpus.append([])

        # Create BM25 model
        if corpus:
            self.bm25_model = BM25Okapi(corpus)
            self.corpus_docs = all_docs
            print(f"✅ BM25 model initialized with {len(corpus)} documents")
        else:
            print("⚠️ No documents found for BM25 initialization")

    def bm25_rerank(self, query: str, retrieved_docs: List[Document], top_k: int = 5) -> List[Document]:
        """
        Rerank documents using BM25 scoring

        Args:
            query: Search query
            retrieved_docs: Documents from initial retrieval
            top_k: Number of top documents to return

        Returns:
            Reranked documents
        """
        if not hasattr(self, 'bm25_model') or not retrieved_docs:
            return retrieved_docs[:top_k]

        print(f"🔄 BM25 reranking {len(retrieved_docs)} documents...")

        # Tokenize query
        query_tokens = query.lower().split()

        # Calculate BM25 scores for retrieved documents
        doc_scores = []
        for doc in retrieved_docs:
            if doc.metadata.get('type') == 'text':
                doc_tokens = doc.page_content.lower().split()
                score = self.bm25_model.get_score(query_tokens, doc_tokens)
            else:
                # For non-text documents, give a default score
                score = 0.1

            doc_scores.append((doc, score))

        # Sort by BM25 score (descending)
        doc_scores.sort(key=lambda x: x[1], reverse=True)

        # Return top-k reranked documents
        reranked_docs = [doc for doc, score in doc_scores[:top_k]]

        print(f"✅ BM25 reranking completed. Top score: {doc_scores[0][1]:.4f}")
        return reranked_docs

    def mmr_rerank(self, query: str, retrieved_docs: List[Document], top_k: int = 5, lambda_param: float = 0.7) -> List[Document]:
        """
        Rerank documents using Maximal Marginal Relevance (MMR)

        Args:
            query: Search query
            retrieved_docs: Documents from initial retrieval
            top_k: Number of documents to return
            lambda_param: Balance between relevance and diversity (0-1)

        Returns:
            MMR reranked documents
        """
        if not retrieved_docs:
            return retrieved_docs[:top_k]

        print(f"🔄 MMR reranking {len(retrieved_docs)} documents with λ={lambda_param}...")

        # Get query embedding
        query_embedding = np.array(self.embeddings.embed_query(query)).reshape(1, -1)

        # Get document embeddings
        doc_embeddings = []
        text_docs = []

        for doc in retrieved_docs:
            if doc.metadata.get('type') == 'text':
                doc_embed = np.array(self.embeddings.embed_query(doc.page_content)).reshape(1, -1)
                doc_embeddings.append(doc_embed)
                text_docs.append(doc)

        if not doc_embeddings:
            return retrieved_docs[:top_k]

        doc_embeddings = np.vstack(doc_embeddings)

        # Calculate relevance scores (cosine similarity with query)
        relevance_scores = cosine_similarity(query_embedding, doc_embeddings)[0]

        # MMR algorithm
        selected_docs = []
        remaining_indices = list(range(len(text_docs)))

        for _ in range(min(top_k, len(text_docs))):
            if not remaining_indices:
                break

            mmr_scores = []

            for i in remaining_indices:
                relevance = relevance_scores[i]

                # Calculate max similarity with already selected documents
                if selected_docs:
                    selected_embeddings = doc_embeddings[[text_docs.index(doc) for doc in selected_docs if doc in text_docs]]
                    if len(selected_embeddings) > 0:
                        max_similarity = np.max(cosine_similarity(doc_embeddings[i:i+1], selected_embeddings))
                    else:
                        max_similarity = 0
                else:
                    max_similarity = 0

                # MMR score: λ * relevance - (1-λ) * max_similarity
                mmr_score = lambda_param * relevance - (1 - lambda_param) * max_similarity
                mmr_scores.append((i, mmr_score))

            # Select document with highest MMR score
            best_idx, best_score = max(mmr_scores, key=lambda x: x[1])
            selected_docs.append(text_docs[best_idx])
            remaining_indices.remove(best_idx)

        # Add any remaining non-text documents
        for doc in retrieved_docs:
            if doc.metadata.get('type') != 'text' and len(selected_docs) < top_k:
                selected_docs.append(doc)

        print(f"✅ MMR reranking completed. Selected {len(selected_docs)} diverse documents")
        return selected_docs[:top_k]

    def hybrid_retrieve_and_rerank(self, query: str, index_type: str = 'FLAT',
                                 rerank_method: str = 'mmr', top_k: int = 5) -> Dict:
        """
        Complete retrieval and reranking pipeline

        Args:
            query: Search query
            index_type: FAISS index type to use
            rerank_method: 'bm25', 'mmr', or 'both'
            top_k: Number of final documents to return

        Returns:
            Dictionary with retrieval and reranking results
        """
        start_time = time.time()

        # Step 1: Initial retrieval
        print(f"🔍 Initial retrieval using {index_type} index...")
        initial_docs = self.pipeline.multivector_retrievers[index_type].get_relevant_documents(query)
        retrieval_time = time.time() - start_time

        # Step 2: Reranking
        rerank_start = time.time()

        if rerank_method == 'bm25':
            final_docs = self.bm25_rerank(query, initial_docs, top_k)
        elif rerank_method == 'mmr':
            final_docs = self.mmr_rerank(query, initial_docs, top_k)
        elif rerank_method == 'both':
            # First BM25, then MMR
            bm25_docs = self.bm25_rerank(query, initial_docs, top_k * 2)
            final_docs = self.mmr_rerank(query, bm25_docs, top_k)
        else:
            final_docs = initial_docs[:top_k]

        rerank_time = time.time() - rerank_start
        total_time = time.time() - start_time

        return {
            'query': query,
            'index_type': index_type,
            'rerank_method': rerank_method,
            'initial_docs': initial_docs,
            'final_docs': final_docs,
            'retrieval_time': retrieval_time,
            'rerank_time': rerank_time,
            'total_time': total_time,
            'initial_count': len(initial_docs),
            'final_count': len(final_docs)
        }

from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class RAGPromptTemplates:
    """Collection of prompt templates for different RAG scenarios"""

    @staticmethod
    def get_comprehensive_rag_template():
        """Comprehensive RAG prompt template"""
        template = """You are an expert AI assistant capable of analyzing and synthesizing information from multiple types of documents including text, images, and tables.

CONTEXT INFORMATION:
{context}

QUERY: {query}

INSTRUCTIONS:
1. Analyze all provided context carefully, including any text content, image descriptions, and table summaries
2. Provide the most informative and relevant answer possible using the context. If context is insufficient, clarify what is missing.
3. If the context contains images or tables, explicitly reference them in your response
4. Structure your response clearly with appropriate headings and bullet points where helpful
5. If you cannot find relevant information in the context, clearly state this limitation
6. Cite specific parts of the context when making claims

RESPONSE REQUIREMENTS:
- Be thorough and detailed in your analysis
- Maintain accuracy and avoid speculation beyond the provided context
- Use clear, professional language
- Include relevant details from images and tables when applicable
- Provide actionable insights when possible

Please provide a comprehensive response to the query based on the context provided."""

        return ChatPromptTemplate.from_template(template)


    @staticmethod
    def get_summary_template():
        """Template for document summarization"""
        template = """Based on the following documents, provide a concise summary:

DOCUMENTS:
{context}

QUERY: {query}

Please provide a clear, structured summary that addresses the query while covering the key points from all document types (text, images, tables)."""

        return ChatPromptTemplate.from_template(template)

    @staticmethod
    def get_qa_template():
        """Template for question answering"""
        template = """Answer the following question based on the provided context. Be specific and cite relevant information.

CONTEXT:
{context}

QUESTION: {query}

ANSWER:"""

        return ChatPromptTemplate.from_template(template)

# Step 11: Generate Output through LLM
class RAGGenerator:
    def __init__(self, reranked_pipeline: RerankedRAGPipeline, llm_model):
        """
        RAG response generator

        Args:
            reranked_pipeline: RerankedRAGPipeline instance
            llm_model: LLM for generation
        """
        self.pipeline = reranked_pipeline
        self.llm = llm_model
        self.prompt_templates = RAGPromptTemplates()

    def format_context(self, documents: List[Document]) -> str:
        """
        Format retrieved documents into context string

        Args:
            documents: Retrieved documents

        Returns:
            Formatted context string
        """
        context_parts = []

        for i, doc in enumerate(documents, 1):
            doc_type = doc.metadata.get('type', 'unknown')

            if doc_type == 'text':
                context_parts.append(f"TEXT DOCUMENT {i}:\n{doc.page_content}\n")
            elif doc_type == 'image':
                filename = doc.metadata.get('filename', 'unknown')
                context_parts.append(f"IMAGE {i} ({filename}):\n[Base64 encoded image - content not directly readable]\nImage context should be interpreted from associated summaries.\n")
            elif doc_type == 'table':
                filename = doc.metadata.get('filename', 'unknown')
                context_parts.append(f"TABLE {i} ({filename}):\n[Base64 encoded table - content not directly readable]\nTable context should be interpreted from associated summaries.\n")
            else:
                context_parts.append(f"DOCUMENT {i} ({doc_type}):\n{doc.page_content}\n")

        return "\n" + "="*50 + "\n".join(context_parts)

    def generate_response(self, query: str, index_type: str = 'FLAT',
                         rerank_method: str = 'mmr', template_type: str = 'comprehensive') -> Dict:
        """
        Generate complete RAG response

        Args:
            query: User query
            index_type: FAISS index type
            rerank_method: Reranking method
            template_type: Type of prompt template to use

        Returns:
            Complete response with timing and metadata
        """
        start_time = time.time()

        # Step 1: Retrieve and rerank documents
        print(f"🚀 Generating RAG response for: '{query[:50]}...'")
        retrieval_result = self.pipeline.hybrid_retrieve_and_rerank(
            query, index_type, rerank_method
        )

        # Step 2: Format context
        context = self.format_context(retrieval_result['final_docs'])

        # Step 3: Select prompt template
        if template_type == 'comprehensive':
            prompt_template = self.prompt_templates.get_comprehensive_rag_template()
        elif template_type == 'summary':
            prompt_template = self.prompt_templates.get_summary_template()
        elif template_type == 'qa':
            prompt_template = self.prompt_templates.get_qa_template()
        else:
            prompt_template = self.prompt_templates.get_comprehensive_rag_template()

        # Step 4: Generate response
        print("🤖 Generating LLM response...")
        generation_start = time.time()

        # Create the chain
        chain = prompt_template | self.llm | StrOutputParser()

        # Generate response
        response = chain.invoke({
            "context": context,
            "query": query
        })

        generation_time = time.time() - generation_start
        total_time = time.time() - start_time

        return {
            'query': query,
            'response': response,
            'retrieval_result': retrieval_result,
            'context': context,
            'template_type': template_type,
            'generation_time': generation_time,
            'total_pipeline_time': total_time,
            'num_source_docs': len(retrieval_result['final_docs']),
            'metadata': {
                'index_type': index_type,
                'rerank_method': rerank_method,
                'retrieval_time': retrieval_result['retrieval_time'],
                'rerank_time': retrieval_result['rerank_time']
            }
        }

from docx import Document as DocxDocument
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.style import WD_STYLE_TYPE
import base64
import io
from PIL import Image
import os
from datetime import datetime

class DOCXRenderer:
    def __init__(self):
        """Initialize DOCX renderer"""
        self.doc = None

    def create_document(self, title: str = "RAG Pipeline Output"):
        """Create a new DOCX document"""
        self.doc = DocxDocument()

        # Add title
        title_para = self.doc.add_heading(title, 0)
        title_para.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Add timestamp
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        timestamp_para = self.doc.add_paragraph(f"Generated on: {timestamp}")
        timestamp_para.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Add separator
        self.doc.add_paragraph("=" * 80)

        return self.doc

    def add_query_section(self, query: str):
        """Add query section to document"""
        if not self.doc:
            self.create_document()

        # Add query heading
        query_heading = self.doc.add_heading("Query", level=1)
        query_heading.alignment = WD_ALIGN_PARAGRAPH.LEFT

        # Add query text
        query_para = self.doc.add_paragraph(query)


         # Set style safely (only paragraph styles)
        paragraph_styles = [s.name for s in self.doc.styles if s.type == WD_STYLE_TYPE.PARAGRAPH]
        if 'Intense Quote' in paragraph_styles:
            query_para.style = 'Intense Quote'
        elif 'Quote' in paragraph_styles:
            query_para.style = 'Quote'
        else:
            query_para.style = 'Normal'  # fallback

        return query_para



    def add_response_section(self, response: str):
        """Add response section to document"""
        if not self.doc:
            self.create_document()

        # Add response heading
        response_heading = self.doc.add_heading("Response", level=1)
        response_heading.alignment = WD_ALIGN_PARAGRAPH.LEFT

        # Split response into paragraphs and add them
        paragraphs = response.split('\n\n')
        for para_text in paragraphs:
            if para_text.strip():
                if para_text.strip().startswith('#'):
                    # Handle markdown-style headers
                    header_level = para_text.count('#')
                    header_text = para_text.replace('#', '').strip()
                    self.doc.add_heading(header_text, level=min(header_level, 6))
                elif para_text.strip().startswith('*') or para_text.strip().startswith('-'):
                    # Handle bullet points
                    bullet_text = para_text.replace('*', '').replace('-', '').strip()
                    bullet_para = self.doc.add_paragraph(bullet_text, style='List Bullet')
                else:
                    # Regular paragraph
                    self.doc.add_paragraph(para_text.strip())

        return response_heading

    def add_metadata_section(self, metadata: Dict):
        """Add metadata section to document"""
        if not self.doc:
            self.create_document()

        # Add metadata heading
        meta_heading = self.doc.add_heading("Pipeline Metadata", level=1)

        # Create metadata table
        table = self.doc.add_table(rows=1, cols=2)
        table.style = 'Table Grid'

        # Add header row
        hdr_cells = table.rows[0].cells
        hdr_cells[0].text = 'Metric'
        hdr_cells[1].text = 'Value'

        # Add metadata rows
        metadata_items = [
            ('Index Type', metadata.get('index_type', 'N/A')),
            ('Reranking Method', metadata.get('rerank_method', 'N/A')),
            ('Retrieval Time (ms)', f"{metadata.get('retrieval_time', 0) * 1000:.2f}"),
            ('Rerank Time (ms)', f"{metadata.get('rerank_time', 0) * 1000:.2f}"),
            ('Generation Time (ms)', f"{metadata.get('generation_time', 0) * 1000:.2f}"),
            ('Total Pipeline Time (ms)', f"{metadata.get('total_pipeline_time', 0) * 1000:.2f}"),
            ('Source Documents', str(metadata.get('num_source_docs', 0))),
            ('Template Type', metadata.get('template_type', 'N/A'))
        ]

        for metric, value in metadata_items:
            row_cells = table.add_row().cells
            row_cells[0].text = metric
            row_cells[1].text = value

        return table

    def add_source_documents_section(self, documents: List[Document]):
        """Add source documents section"""
        if not self.doc:
            self.create_document()

        # Add sources heading
        sources_heading = self.doc.add_heading("Source Documents", level=1)

        for i, doc in enumerate(documents, 1):
            doc_type = doc.metadata.get('type', 'unknown')

            # Add document subheading
            doc_heading = self.doc.add_heading(f"Document {i}: {doc_type.title()}", level=2)

            if doc_type == 'text':
                # Add text content
                content_para = self.doc.add_paragraph(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                content_para.style = 'Normal'

            elif doc_type in ['image', 'table']:
                # Add placeholder for images/tables
                filename = doc.metadata.get('filename', 'unknown')
                placeholder_para = self.doc.add_paragraph(f"[{doc_type.title()}: {filename}]")
                applied = False
                for style_name in ['Subtle Emphasis', 'Intense Quote', 'Normal']:
                  try:
                    style_obj = self.doc.styles[style_name]
                    if style_obj.type == WD_STYLE_TYPE.PARAGRAPH:
                      placeholder_para.style = style_obj
                      applied = True
                      break
                  except KeyError:
                      continue  # Style not found

                if not applied:
                  placeholder_para.style = self.doc.styles['Normal']

                # Try to decode and add image if it's base64
                if doc_type == 'image':
                    try:
                        # Decode base64 image
                        image_data = base64.b64decode(doc.page_content)
                        image_stream = io.BytesIO(image_data)

                        # Add image to document
                        self.doc.add_picture(image_stream, width=Inches(4))

                    except Exception as e:
                        error_para = self.doc.add_paragraph(f"[Could not render image: {str(e)}]")
                        applied = False
                        for style_name in ['Subtle Emphasis', 'Intense Quote', 'Normal']:
                          try:
                            style_obj = self.doc.styles[style_name]
                            if style_obj.type == WD_STYLE_TYPE.PARAGRAPH:
                               placeholder_para.style = style_obj
                               applied = True
                               break
                          except KeyError:
                              continue  # Style not found

                        if not applied:
                           placeholder_para.style = self.doc.styles['Normal']

            # Add separator
            self.doc.add_paragraph("─" * 40)

        return sources_heading

    def add_performance_comparison(self, benchmark_results: Dict):
        """Add performance comparison section"""
        if not self.doc:
            self.create_document()

        # Add performance heading
        perf_heading = self.doc.add_heading("Performance Analysis", level=1)

        # Create performance table
        table = self.doc.add_table(rows=1, cols=5)
        table.style = 'Table Grid'

        # Add header row
        hdr_cells = table.rows[0].cells
        headers = ['Index Type', 'Mean Time (ms)', 'Throughput (QPS)', 'Std Dev (ms)', 'Total Retrievals']
        for i, header in enumerate(headers):
            hdr_cells[i].text = header

        # Add performance data
        if 'aggregated' in benchmark_results:
            for index_type, stats in benchmark_results['aggregated'].items():
                row_cells = table.add_row().cells
                row_cells[0].text = index_type
                row_cells[1].text = f"{stats['mean_time'] * 1000:.2f}"
                row_cells[2].text = f"{stats['throughput_qps']:.1f}"
                row_cells[3].text = f"{stats['std_dev'] * 1000:.2f}"
                row_cells[4].text = str(stats['total_retrievals'])

        return table

    def render_complete_rag_output(self, rag_result: Dict, benchmark_results: Dict = None,
                                  filename: str = "rag_output.docx") -> str:
        """
        Render complete RAG output to DOCX file

        Args:
            rag_result: Result from RAG generation
            benchmark_results: Optional benchmark results
            filename: Output filename

        Returns:
            Path to saved DOCX file
        """
        # Create new document
        self.create_document("RAG Pipeline Complete Output")

        # Add query section
        self.add_query_section(rag_result['query'])

        # Add response section
        self.add_response_section(rag_result['response'])

        # Add metadata section
        metadata = {
            'index_type': rag_result['metadata']['index_type'],
            'rerank_method': rag_result['metadata']['rerank_method'],
            'retrieval_time': rag_result['metadata']['retrieval_time'],
            'rerank_time': rag_result['metadata']['rerank_time'],
            'generation_time': rag_result['generation_time'],
            'total_pipeline_time': rag_result['total_pipeline_time'],
            'num_source_docs': rag_result['num_source_docs'],
            'template_type': rag_result['template_type']
        }
        self.add_metadata_section(metadata)

        # Add source documents section
        self.add_source_documents_section(rag_result['retrieval_result']['final_docs'])

        # Add performance comparison if provided
        if benchmark_results:
            self.add_performance_comparison(benchmark_results)

        # Save document
        self.doc.save(filename)

        return filename

    # def create_batch_report(self, batch_results: List[Dict], benchmark_results: Dict = None,
    #                        filename: str = "batch_rag_report.docx") -> str:
    #     """
    #     Create a batch report for multiple RAG results

    #     Args:
    #         batch_results: List of RAG results
    #         benchmark_results: Optional benchmark results
    #         filename: Output filename

    #     Returns:
    #         Path to saved DOCX file
    #     """
    #     # Create new document
    #     self.create_document("RAG Pipeline Batch Report")

    #     # Add summary section
    #     summary_heading = self.doc.add_heading("Executive Summary", level=1)
    #     summary_para = self.doc.add_paragraph(f"This report contains {len(batch_results)} RAG pipeline results with comprehensive analysis.")

    #     # Add each result
    #     for i, result in enumerate(batch_results, 1):
    #         # Add page break
    #         self.doc.add_page_break()

    #         # Add result heading
    #         result_heading = self.doc.add_heading(f"Result {i}", level=1)

    #         # Add query
    #         self.add_query_section(result['query'])

    #         # Add response
    #         self.add_response_section(result['response'])

    #         # Add metadata
    #         metadata = {
    #             'index_type': result['metadata']['index_type'],
    #             'rerank_method': result['metadata']['rerank_method'],
    #             'retrieval_time': result['metadata']['retrieval_time'],
    #             'rerank_time': result['metadata']['rerank_time'],
    #             'generation_time': result['generation_time'],
    #             'total_pipeline_time': result['total_pipeline_time'],
    #             'num_source_docs': result['num_source_docs'],
    #             'template_type': result['template_type']
    #         }
    #         self.add_metadata_section(metadata)

    #     # Add performance comparison at the end
    #     if benchmark_results:
    #         self.doc.add_page_break()
    #         self.add_performance_comparison(benchmark_results)

    #     # Save document
    #     self.doc.save(filename)

    #     return filename

# Complete Implementation and Testing
print("🚀 Initializing complete RAG pipeline with reranking and DOCX rendering...")

# Initialize the enhanced pipeline
reranked_pipeline = RerankedRAGPipeline(multivector_pipeline, model)
rag_generator = RAGGenerator(reranked_pipeline, model)
docx_renderer = DOCXRenderer()

# Test the complete pipeline
print("🧪 Testing complete RAG pipeline...")

# Test query
test_query = "explain me about this closed book open book model from the table whats its saying"

# Generate RAG response
rag_result = rag_generator.generate_response(
    query=test_query,
    index_type='FLAT',
    rerank_method='mmr',
    template_type='comprehensive'
)

print(f"✅ RAG Response Generated!")
print(f"📊 Pipeline Performance:")
print(f"  - Retrieval Time: {rag_result['metadata']['retrieval_time']:.4f}s")
print(f"  - Reranking Time: {rag_result['metadata']['rerank_time']:.4f}s")
print(f"  - Generation Time: {rag_result['generation_time']:.4f}s")
print(f"  - Total Time: {rag_result['total_pipeline_time']:.4f}s")
print(f"  - Source Documents: {rag_result['num_source_docs']}")

# Render to DOCX
print("📄 Rendering to DOCX...")
docx_filename = docx_renderer.render_complete_rag_output(
    rag_result=rag_result,
    benchmark_results=benchmark_results,
    filename=f"rag_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
)

print(f"✅ DOCX file created: {docx_filename}")

# Performance summary
print(f"\n📈 FINAL PERFORMANCE SUMMARY:")
print(f"═══════════════════════════════════════")

print(f"Total Queries Processed: 1")
print(f"Retrieval Time: {rag_result['metadata']['retrieval_time']:.4f}s")
print(f"Reranking Time: {rag_result['metadata']['rerank_time']:.4f}s")
print(f"Generation Time: {rag_result['generation_time']:.4f}s")
print(f"Total Pipeline Time: {rag_result['total_pipeline_time']:.4f}s")

print(f"\n🎯 PIPELINE COMPONENTS COMPLETED:")
print(f"✅ Step 9: Reranking (BM25 + MMR)")
print(f"✅ Step 10: Prompt Templates")
print(f"✅ Step 11: LLM Generation")
print(f"✅ Step 12: DOCX Rendering")

print(f"\n📁 Generated File:")
print(f"  - Single RAG Output: {docx_filename}")

print(f"\n🏆 RAG Pipeline Implementation Complete!")